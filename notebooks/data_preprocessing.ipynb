{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Quality Dataset - Data Preprocessing\n",
    "# ========================================\n",
    "\n",
    "This notebook handles data cleaning, feature engineering, and preprocessing for the wine quality prediction system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom processor\n",
    "from data_processor import WineQualityProcessor\n",
    "\n",
    "print(\"Wine Quality Dataset - Data Preprocessing\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize processor\n",
    "processor = WineQualityProcessor(random_state=42)\n",
    "\n",
    "# Load raw data\n",
    "raw_data = processor.load_data()\n",
    "\n",
    "print(f\"Raw data shape: {raw_data.shape}\")\n",
    "print(f\"Features: {list(raw_data.columns)}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "display(raw_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data quality\n",
    "print(\"DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "missing_values = raw_data.isnull().sum()\n",
    "print(missing_values[missing_values > 0] if missing_values.sum() > 0 else \"No missing values found âœ“\")\n",
    "\n",
    "# Duplicates\n",
    "duplicates = raw_data.duplicated().sum()\n",
    "print(f\"\\nDuplicate rows: {duplicates}\")\n",
    "\n",
    "# Data types\n",
    "print(\"\\nData types:\")\n",
    "print(raw_data.dtypes)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nBasic statistics:\")\n",
    "display(raw_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Outlier Detection and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect outliers using IQR method\n",
    "def detect_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return len(outliers), lower_bound, upper_bound\n",
    "\n",
    "# Analyze outliers for numerical columns\n",
    "numerical_cols = raw_data.select_dtypes(include=[np.number]).columns\n",
    "numerical_cols = [col for col in numerical_cols if col != 'quality']\n",
    "\n",
    "print(\"OUTLIER ANALYSIS (IQR Method)\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "outlier_info = []\n",
    "for col in numerical_cols:\n",
    "    outlier_count, lower, upper = detect_outliers(raw_data, col)\n",
    "    outlier_percentage = (outlier_count / len(raw_data)) * 100\n",
    "    outlier_info.append({\n",
    "        'Feature': col,\n",
    "        'Outliers': outlier_count,\n",
    "        'Percentage': f\"{outlier_percentage:.1f}%\",\n",
    "        'Lower_Bound': f\"{lower:.3f}\",\n",
    "        'Upper_Bound': f\"{upper:.3f}\"\n",
    "    })\n",
    "    \n",
    "outlier_df = pd.DataFrame(outlier_info)\n",
    "display(outlier_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    if i < len(axes):\n",
    "        raw_data.boxplot(column=col, ax=axes[i])\n",
    "        axes[i].set_title(f'{col} - Outliers')\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Remove empty subplots\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "cleaned_data = processor.clean_data(raw_data)\n",
    "\n",
    "print(f\"Data shape after cleaning: {cleaned_data.shape}\")\n",
    "print(f\"Rows removed: {len(raw_data) - len(cleaned_data)}\")\n",
    "\n",
    "# Show cleaning results\n",
    "print(\"\\nData cleaning completed:\")\n",
    "print(f\"âœ“ Missing values handled\")\n",
    "print(f\"âœ“ Duplicates removed\")\n",
    "print(f\"âœ“ Data types verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional features\n",
    "featured_data = cleaned_data.copy()\n",
    "\n",
    "# Quality categories for analysis\n",
    "featured_data['quality_category'] = pd.cut(\n",
    "    featured_data['quality'], \n",
    "    bins=[0, 4, 6, 10], \n",
    "    labels=['low', 'medium', 'high'],\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "# Alcohol strength categories\n",
    "featured_data['alcohol_strength'] = pd.cut(\n",
    "    featured_data['alcohol'],\n",
    "    bins=[0, 10, 12, 20],\n",
    "    labels=['low', 'medium', 'high'],\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "# Acidity ratio\n",
    "featured_data['acidity_ratio'] = (\n",
    "    featured_data['fixed acidity'] / (featured_data['volatile acidity'] + 0.001)\n",
    ")\n",
    "\n",
    "# Sulfur dioxide ratio\n",
    "featured_data['sulfur_ratio'] = (\n",
    "    featured_data['free sulfur dioxide'] / (featured_data['total sulfur dioxide'] + 0.001)\n",
    ")\n",
    "\n",
    "print(\"Feature Engineering Results:\")\n",
    "print(f\"Original features: {len(cleaned_data.columns)}\")\n",
    "print(f\"Total features after engineering: {len(featured_data.columns)}\")\n",
    "print(f\"New features created: {len(featured_data.columns) - len(cleaned_data.columns)}\")\n",
    "\n",
    "print(\"\\nNew features:\")\n",
    "new_features = ['quality_category', 'alcohol_strength', 'acidity_ratio', 'sulfur_ratio']\n",
    "for feature in new_features:\n",
    "    print(f\"âœ“ {feature}\")\n",
    "\n",
    "display(featured_data[new_features].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Scaling and Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "target = featured_data['quality'].copy()\n",
    "exclude_cols = ['quality', 'quality_category', 'alcohol_strength']  # Exclude categorical analysis features\n",
    "features = featured_data.drop(columns=exclude_cols)\n",
    "\n",
    "print(f\"Features for modeling: {list(features.columns)}\")\n",
    "print(f\"Target variable: quality\")\n",
    "\n",
    "# Handle categorical variables (wine_type)\n",
    "categorical_cols = features.select_dtypes(include=['object', 'category']).columns\n",
    "print(f\"\\nCategorical features: {list(categorical_cols)}\")\n",
    "\n",
    "if len(categorical_cols) > 0:\n",
    "    label_encoder = LabelEncoder()\n",
    "    for col in categorical_cols:\n",
    "        features[col] = label_encoder.fit_transform(features[col])\n",
    "        print(f\"âœ“ Encoded {col}: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
    "\n",
    "# Display feature statistics before scaling\n",
    "print(\"\\nFeature statistics before scaling:\")\n",
    "display(features.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    features, target, test_size=0.4, random_state=42, stratify=target\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Data Split Results:\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(features)*100:.1f}%)\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(features)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(features)*100:.1f}%)\")\n",
    "print(f\"Total: {len(features)} samples\")\n",
    "\n",
    "# Check target distribution in splits\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(\"Training set:\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "print(\"\\nValidation set:\")\n",
    "print(y_val.value_counts().sort_index())\n",
    "print(\"\\nTest set:\")\n",
    "print(y_test.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data and transform all sets\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "X_val_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_val),\n",
    "    columns=X_val.columns,\n",
    "    index=X_val.index\n",
    ")\n",
    "\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test),\n",
    "    columns=X_test.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "print(\"Feature Scaling Results:\")\n",
    "print(f\"âœ“ Training set scaled: {X_train_scaled.shape}\")\n",
    "print(f\"âœ“ Validation set scaled: {X_val_scaled.shape}\")\n",
    "print(f\"âœ“ Test set scaled: {X_test_scaled.shape}\")\n",
    "\n",
    "print(\"\\nScaled feature statistics (training set):\")\n",
    "display(X_train_scaled.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Dataset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FINAL PREPROCESSING SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"âœ“ Data loaded: {raw_data.shape[0]} samples\")\n",
    "print(f\"âœ“ Data cleaned: {len(raw_data) - len(cleaned_data)} rows removed\")\n",
    "print(f\"âœ“ Features engineered: {len(featured_data.columns) - len(cleaned_data.columns)} new features\")\n",
    "print(f\"âœ“ Categorical variables encoded: {len(categorical_cols)} features\")\n",
    "print(f\"âœ“ Data split: {len(X_train)}/{len(X_val)}/{len(X_test)} (train/val/test)\")\n",
    "print(f\"âœ“ Features scaled: StandardScaler applied\")\n",
    "\n",
    "print(f\"\\nFinal dataset characteristics:\")\n",
    "print(f\"â€¢ Features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"â€¢ Samples: {len(features)}\")\n",
    "print(f\"â€¢ Target classes: {sorted(target.unique())}\")\n",
    "print(f\"â€¢ Feature names: {list(X_train_scaled.columns)}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Data is ready for model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Processed Data (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data for model training\n",
    "import os\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# Save processed datasets\n",
    "X_train_scaled.to_csv('data/processed/X_train.csv', index=False)\n",
    "X_val_scaled.to_csv('data/processed/X_val.csv', index=False)\n",
    "X_test_scaled.to_csv('data/processed/X_test.csv', index=False)\n",
    "y_train.to_csv('data/processed/y_train.csv', index=False)\n",
    "y_val.to_csv('data/processed/y_val.csv', index=False)\n",
    "y_test.to_csv('data/processed/y_test.csv', index=False)\n",
    "\n",
    "print(\"âœ“ Processed datasets saved to 'data/processed/' directory\")\n",
    "print(\"âœ“ Ready for model training phase\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}